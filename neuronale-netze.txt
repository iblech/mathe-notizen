=== LSTMs

http://colah.github.io/posts/2015-08-Understanding-LSTMs/


=== Wieso waren neuronale Netze nicht schon Anfang 2000 eine große Sache?

* Rechenkapazität
* Convolutional Neuronal Network
* RELU-Aktivierungsfunktion
* Batch-Normalisierung
* Zufällige Initialisierung
* Mehr und bessere Testdaten

https://plus.google.com/+DanPiponi/posts/fguCASnATcD


=== Wie man die Strassensche Regel für Matrixmultiplikation findet

http://arxiv.org/pdf/1601.07227.pdf


=== Über die Wahl der richtigen Aktivierungsfunktion

http://cs231n.github.io/neural-networks-1/#actfun


=== Kontinuierliche Varianten neuronaler Netze

* https://arxiv.org/abs/1806.07366


=== Reasoning models don't always say what they think

https://www.anthropic.com/research/reasoning-models-dont-say-think


=== KI-Erfolge in Mathematik

https://news.ycombinator.com/item?id=44613840
OpenAI, IMO 2025


=== Gemischtes

* Alloys: https://xbow.com/blog/alloy-agents/

* Embedding: https://huggingface.co/spaces/hesamation/primer-llm-embedding?section=word2vec

* Wie denken multilinguale LLMs? https://arxiv.org/pdf/2408.10811

* Floor raiser, not ceiling raiser: https://elroy.bot/blog/2025/07/29/ai-is-a-floor-raiser-not-a-ceiling-raiser.html#creative-works-not-coming-to-a-theater-near-you

* Architekturüberblick: https://magazine.sebastianraschka.com/p/from-gpt-2-to-gpt-oss-analyzing-the

* Training language models to be warm and empathetic makes them less reliable:
  https://news.ycombinator.com/item?id=44875992

* Reinforcement learning: https://news.ycombinator.com/item?id=44929424

* Min-p sampling als bessere Alternative zu Top-k und Max-p:
  https://openreview.net/pdf?id=FBkpCyujtS

* Zweistufiges LLM-System gegen Prompt Injection:
  https://simonwillison.net/2025/Apr/11/camel/

* Philosophie: "with agents, do you think we won’t have to learn anything about
  software anymore? And I say, no, no, when I write a prompt for the agent,
  everything I know about developing software comes together"
  https://ampcode.com/how-i-use-amp

* NanoChat, ein ChatGPT-Klon für Lehrzwecke:
  https://nitter.net/karpathy/status/1977755427569111362

* "The tokenizer must go": https://xcancel.com/karpathy/status/1980397031542989305

* Introspection: https://transformer-circuits.pub/2025/introspection/index.html

* https://media.ccc.de/v/39c3-breaking-bots-cheating-at-blue-team-ctfs-with-ai-speed-runs

* From Zero to Hero: https://news.ycombinator.com/item?id=46485090

* Fundamental LLM phenomena (ICL, repetition, serial position biases, consequences of RL-based reasoning etc)
  https://news.ycombinator.com/item?id=46641064

* Beispiel für langes Reasoning bei einer Mathe-Frage:
  https://www.kimi.com/share/19bcfe2e-d9a2-81fe-8000-00002163c26c


=== Bücher

* klar, Nielsen
* http://ciml.info/
* https://www.sscardapane.it/alice-book
* Schöne Visualisierung: https://github.com/lilipads/gradient_descent_viz
* Llama3 https://github.com/naklecha/llama3-from-scratch
* https://www.gilesthomas.com/2024/12/llm-from-scratch-1
* https://arxiv.org/pdf/2404.17625
